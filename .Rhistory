,date_var = "DATE" # date format must be "2020-01-01"
,dep_var = "revenue" # there should be only one dependent variable
,dep_var_type = "revenue" # "revenue" or "conversion"
# "trend","season", "weekday", "holiday" are provided and case-sensitive.
# Recommended to at least keep Trend & Holidays
,prophet_vars = c("trend", "season", "holiday")
# c("default", "positive", and "negative"). Recommend as default.
# Must be same length as prophet_vars
,prophet_signs = c("default","default", "default")
# only one country allowed once. Including national holidays for 59 countries,
# whose list can be found on our githut guide
,prophet_country = "DE"
# typically competitors, price & promotion, temperature,  unemployment rate etc
,context_vars = c("competitor_sales_B", "events")
# c("default", "positive", and "negative"), control the signs of coefficients for baseline variables
,context_signs = c("default", "default")
# c("tv_S"	,"ooh_S",	"print_S"	,"facebook_I", "facebook_S"	,"search_clicks_P"	,"search_S").
# We recommend to use media exposure metrics like impressions, GRP etc for the model.
# If not applicable, use spend instead
,paid_media_vars = c("tv_S","ooh_S"	,	"print_S"	,"facebook_I"	,"search_clicks_P")
# c("default", "positive", and "negative"). must have same length as paid_media_vars.
# control the signs of coefficients for media variables
,paid_media_signs = c("positive", "positive","positive", "positive", "positive")
# spends must have same order and same length as paid_media_vars
,paid_media_spends = c("tv_S","ooh_S",	"print_S"	,"facebook_S"	,"search_S")
,organic_vars = c("newsletter")
,organic_signs = c("positive") # must have same length as organic_vars
,factor_vars = c("events") # specify which variables in context_vars and organic_vars are factorial
##############################
#### set model parameters ####
## set cores for parallel computing
,cores = 6 # I am using 6 cores from 8 on my local machine. Use detectCores() to find out cores
## set rolling window start
,window_start = "2016-11-23"
,window_end = "2018-08-22"
## set model core features
# Weibull is more flexible, yet has one more parameter and thus takes longer
,adstock = "geometric" # geometric or weibull
,iterations = 100  # number of allowed iterations per trial. 500 is recommended
# selected algorithm for Nevergrad, the gradient-free optimisation library
# https://facebookresearch.github.io/nevergrad/index.html
,nevergrad_algo = "TwoPointsDE"
# 40 is recommended without calibration, 100 with calibration.
,trials = 2 # number of allowed iterations per trial
## Time estimation: with geometric adstock, 2000 iterations * 5 trials and 6 cores,
# it takes about 30 minutes. Weibull takes at least twice as much time.
#,hyperparameters = hyperparameters
# ,calibration_input = data.table(channel = c("facebook_I",  "tv_S", "facebook_I"),
#                        liftStartDate = as.Date(c("2018-05-01", "2017-11-27", "2018-07-01")),
#                        liftEndDate = as.Date(c("2018-06-10", "2017-12-03", "2018-07-20")),
#                        liftAbs = c(400000, 300000, 200000))
)
library(Robyn)
library(Robyn)
InputCollect <- robyn_inputs(
dt_input = dt_simulated_weekly
,dt_holidays = dt_prophet_holidays
#######################
#### set variables ####
,date_var = "DATE" # date format must be "2020-01-01"
,dep_var = "revenue" # there should be only one dependent variable
,dep_var_type = "revenue" # "revenue" or "conversion"
# "trend","season", "weekday", "holiday" are provided and case-sensitive.
# Recommended to at least keep Trend & Holidays
,prophet_vars = c("trend", "season", "holiday")
# c("default", "positive", and "negative"). Recommend as default.
# Must be same length as prophet_vars
,prophet_signs = c("default","default", "default")
# only one country allowed once. Including national holidays for 59 countries,
# whose list can be found on our githut guide
,prophet_country = "DE"
# typically competitors, price & promotion, temperature,  unemployment rate etc
,context_vars = c("competitor_sales_B", "events")
# c("default", "positive", and "negative"), control the signs of coefficients for baseline variables
,context_signs = c("default", "default")
# c("tv_S"	,"ooh_S",	"print_S"	,"facebook_I", "facebook_S"	,"search_clicks_P"	,"search_S").
# We recommend to use media exposure metrics like impressions, GRP etc for the model.
# If not applicable, use spend instead
,paid_media_vars = c("tv_S","ooh_S"	,	"print_S"	,"facebook_I"	,"search_clicks_P")
# c("default", "positive", and "negative"). must have same length as paid_media_vars.
# control the signs of coefficients for media variables
,paid_media_signs = c("positive", "positive","positive", "positive", "positive")
# spends must have same order and same length as paid_media_vars
,paid_media_spends = c("tv_S","ooh_S",	"print_S"	,"facebook_S"	,"search_S")
,organic_vars = c("newsletter")
,organic_signs = c("positive") # must have same length as organic_vars
,factor_vars = c("events") # specify which variables in context_vars and organic_vars are factorial
##############################
#### set model parameters ####
## set cores for parallel computing
,cores = 6 # I am using 6 cores from 8 on my local machine. Use detectCores() to find out cores
## set rolling window start
,window_start = "2016-11-23"
,window_end = "2018-08-22"
## set model core features
# Weibull is more flexible, yet has one more parameter and thus takes longer
,adstock = "geometric" # geometric or weibull
,iterations = 100  # number of allowed iterations per trial. 500 is recommended
# selected algorithm for Nevergrad, the gradient-free optimisation library
# https://facebookresearch.github.io/nevergrad/index.html
,nevergrad_algo = "TwoPointsDE"
# 40 is recommended without calibration, 100 with calibration.
,trials = 2 # number of allowed iterations per trial
## Time estimation: with geometric adstock, 2000 iterations * 5 trials and 6 cores,
# it takes about 30 minutes. Weibull takes at least twice as much time.
#,hyperparameters = hyperparameters
# ,calibration_input = data.table(channel = c("facebook_I",  "tv_S", "facebook_I"),
#                        liftStartDate = as.Date(c("2018-05-01", "2017-11-27", "2018-07-01")),
#                        liftEndDate = as.Date(c("2018-06-10", "2017-12-03", "2018-07-20")),
#                        liftAbs = c(400000, 300000, 200000))
)
# helper plots: set plot to TRUE for transformation examples:
# adstock transformation example plot, helping you understand geometric/theta and weibull/shape/scale transformation
plot_adstock(FALSE)
# s-curve transformation example plot, helping you understand hill/alpha/gamma transformatio
plot_saturation(FALSE)
hyper_names(adstock = InputCollect$adstock, all_media = InputCollect$all_media)
hyperparameters <- list(
facebook_I_alphas = c(0.5, 3) # example bounds for alpha
,facebook_I_gammas = c(0.3, 1) # example bounds for gamma
,facebook_I_thetas = c(0, 0.3) # example bounds for theta
#,facebook_I_shapes = c(0.0001, 2) # example bounds for shape
#,facebook_I_scales = c(0, 0.1) # example bounds for scale
,print_S_alphas = c(0.5, 3)
,print_S_gammas = c(0.3, 1)
,print_S_thetas = c(0.1, 0.4)
#,print_S_shapes = c(0.0001, 2)
#,print_S_scales = c(0, 0.1)
,tv_S_alphas = c(0.5, 3)
,tv_S_gammas = c(0.3, 1)
,tv_S_thetas = c(0.3, 0.8)
#,tv_S_shapes = c(0.0001, 2)
#,tv_S_scales= c(0, 0.1)
,search_clicks_P_alphas = c(0.5, 3)
,search_clicks_P_gammas = c(0.3, 1)
,search_clicks_P_thetas = c(0, 0.3)
#,search_clicks_P_shapes = c(0.0001, 2)
#,search_clicks_P_scales = c(0, 0.1)
,ooh_S_alphas = c(0.5, 3)
,ooh_S_gammas = c(0.3, 1)
,ooh_S_thetas = c(0.1, 0.4)
#,ooh_S_shapes = c(0.0001, 2)
#,ooh_S_scales = c(0, 0.1)
,newsletter_alphas = c(0.5, 3)
,newsletter_gammas = c(0.3, 1)
,newsletter_thetas = c(0.1, 0.4)
#,newsletter_shapes = c(0.0001, 2)
#,newsletter_scales = c(0, 0.1)
)
InputCollect <- robyn_inputs(InputCollect = InputCollect
, hyperparameters = hyperparameters)
?add_regressor
devtools::document()
devtools::document()
library(Robyn)
InputCollect <- robyn_inputs(InputCollect = InputCollect
, hyperparameters = hyperparameters)
OutputCollect <- robyn_run(InputCollect = InputCollect
, plot_folder = robyn_object
, pareto_fronts = 1
, plot_pareto = TRUE)
?registerDoFuture
??registerDoFuture
library(Robyn)
OutputCollect <- robyn_run(InputCollect = InputCollect
, plot_folder = robyn_object
, pareto_fronts = 1
, plot_pareto = TRUE)
library(Robyn)
## Install and load libraries
library(Robyn) # devtools::install_github("facebookexperimental/Robyn@package_test")
## Check simulatetd dataset or load your own dataset
data("dt_simulated_weekly")
head(dt_simulated_weekly)
## Check holidays from Prophet
# 59 countries included. If your country is not included, please manually add it.
# Tipp: any events can be added into this table, school break, events etc.
data("dt_prophet_holidays")
head(dt_prophet_holidays)
## Set robyn_object. It must has extension .RData. The object name can be different than Robyn
robyn_object <- "~/Desktop/Robyn.RData"
InputCollect <- robyn_inputs(
dt_input = dt_simulated_weekly
,dt_holidays = dt_prophet_holidays
#######################
#### set variables ####
,date_var = "DATE" # date format must be "2020-01-01"
,dep_var = "revenue" # there should be only one dependent variable
,dep_var_type = "revenue" # "revenue" or "conversion"
# "trend","season", "weekday", "holiday" are provided and case-sensitive.
# Recommended to at least keep Trend & Holidays
,prophet_vars = c("trend", "season", "holiday")
# c("default", "positive", and "negative"). Recommend as default.
# Must be same length as prophet_vars
,prophet_signs = c("default","default", "default")
# only one country allowed once. Including national holidays for 59 countries,
# whose list can be found on our githut guide
,prophet_country = "DE"
# typically competitors, price & promotion, temperature,  unemployment rate etc
,context_vars = c("competitor_sales_B", "events")
# c("default", "positive", and "negative"), control the signs of coefficients for baseline variables
,context_signs = c("default", "default")
# c("tv_S"	,"ooh_S",	"print_S"	,"facebook_I", "facebook_S"	,"search_clicks_P"	,"search_S").
# We recommend to use media exposure metrics like impressions, GRP etc for the model.
# If not applicable, use spend instead
,paid_media_vars = c("tv_S","ooh_S"	,	"print_S"	,"facebook_I"	,"search_clicks_P")
# c("default", "positive", and "negative"). must have same length as paid_media_vars.
# control the signs of coefficients for media variables
,paid_media_signs = c("positive", "positive","positive", "positive", "positive")
# spends must have same order and same length as paid_media_vars
,paid_media_spends = c("tv_S","ooh_S",	"print_S"	,"facebook_S"	,"search_S")
,organic_vars = c("newsletter")
,organic_signs = c("positive") # must have same length as organic_vars
,factor_vars = c("events") # specify which variables in context_vars and organic_vars are factorial
##############################
#### set model parameters ####
## set cores for parallel computing
,cores = 6 # I am using 6 cores from 8 on my local machine. Use detectCores() to find out cores
## set rolling window start
,window_start = "2016-11-23"
,window_end = "2018-08-22"
## set model core features
# Weibull is more flexible, yet has one more parameter and thus takes longer
,adstock = "geometric" # geometric or weibull
,iterations = 100  # number of allowed iterations per trial. 500 is recommended
# selected algorithm for Nevergrad, the gradient-free optimisation library
# https://facebookresearch.github.io/nevergrad/index.html
,nevergrad_algo = "TwoPointsDE"
# 40 is recommended without calibration, 100 with calibration.
,trials = 2 # number of allowed iterations per trial
## Time estimation: with geometric adstock, 2000 iterations * 5 trials and 6 cores,
# it takes about 30 minutes. Weibull takes at least twice as much time.
#,hyperparameters = hyperparameters
# ,calibration_input = data.table(channel = c("facebook_I",  "tv_S", "facebook_I"),
#                        liftStartDate = as.Date(c("2018-05-01", "2017-11-27", "2018-07-01")),
#                        liftEndDate = as.Date(c("2018-06-10", "2017-12-03", "2018-07-20")),
#                        liftAbs = c(400000, 300000, 200000))
)
# helper plots: set plot to TRUE for transformation examples:
# adstock transformation example plot, helping you understand geometric/theta and weibull/shape/scale transformation
plot_adstock(FALSE)
# s-curve transformation example plot, helping you understand hill/alpha/gamma transformatio
plot_saturation(FALSE)
hyper_names(adstock = InputCollect$adstock, all_media = InputCollect$all_media)
hyperparameters <- list(
facebook_I_alphas = c(0.5, 3) # example bounds for alpha
,facebook_I_gammas = c(0.3, 1) # example bounds for gamma
,facebook_I_thetas = c(0, 0.3) # example bounds for theta
#,facebook_I_shapes = c(0.0001, 2) # example bounds for shape
#,facebook_I_scales = c(0, 0.1) # example bounds for scale
,print_S_alphas = c(0.5, 3)
,print_S_gammas = c(0.3, 1)
,print_S_thetas = c(0.1, 0.4)
#,print_S_shapes = c(0.0001, 2)
#,print_S_scales = c(0, 0.1)
,tv_S_alphas = c(0.5, 3)
,tv_S_gammas = c(0.3, 1)
,tv_S_thetas = c(0.3, 0.8)
#,tv_S_shapes = c(0.0001, 2)
#,tv_S_scales= c(0, 0.1)
,search_clicks_P_alphas = c(0.5, 3)
,search_clicks_P_gammas = c(0.3, 1)
,search_clicks_P_thetas = c(0, 0.3)
#,search_clicks_P_shapes = c(0.0001, 2)
#,search_clicks_P_scales = c(0, 0.1)
,ooh_S_alphas = c(0.5, 3)
,ooh_S_gammas = c(0.3, 1)
,ooh_S_thetas = c(0.1, 0.4)
#,ooh_S_shapes = c(0.0001, 2)
#,ooh_S_scales = c(0, 0.1)
,newsletter_alphas = c(0.5, 3)
,newsletter_gammas = c(0.3, 1)
,newsletter_thetas = c(0.1, 0.4)
#,newsletter_shapes = c(0.0001, 2)
#,newsletter_scales = c(0, 0.1)
)
InputCollect <- robyn_inputs(InputCollect = InputCollect
, hyperparameters = hyperparameters)
OutputCollect <- robyn_run(InputCollect = InputCollect
, plot_folder = robyn_object
, pareto_fronts = 1
, plot_pareto = TRUE)
?doFuture
?registerDoFuture
??registerDoFuture
install.packages("v")
install.packages("doFuture")
install.packages("doFuture")
library(doFuture)
doFuture::registerDoFuture()
devtools::document()
devtools::document()
library(Robyn)
OutputCollect <- robyn_run(InputCollect = InputCollect
, plot_folder = robyn_object
, pareto_fronts = 1
, plot_pareto = TRUE)
OutputCollect$allSolutions
select_model <- "2_12_1"
robyn_save(robyn_object = robyn_object
, select_model = select_model
, InputCollect = InputCollect
, OutputCollect = OutputCollect)
# Check media summary for selected model
OutputCollect$xDecompAgg[solID == select_model & !is.na(mean_spend),
.(rn, coef,mean_spend, mean_response, roi_mean, total_spend,
total_response = xDecompAgg, roi_total, solID)]
AllocatorCollect <- robyn_allocator(
InputCollect = InputCollect
,OutputCollect = OutputCollect
# input one of the model IDs in model_output_collect$allSolutions to get optimization result
,select_model = select_model
,optim_algo = "SLSQP_AUGLAG" # "MMA_AUGLAG", "SLSQP_AUGLAG"
,scenario = "max_historical_response" # c(max_historical_response, max_response_expected_spend)
# specify future spend volume. only applies when scenario = "max_response_expected_spend"
#,expected_spend = 100000
# specify period for the future spend volume in days. Only applies when scenario = "max_response_expected_spend"
#,expected_spend_days = 90
# must be between 0.01-1 and has same length and order as paid_media_vars
,channel_constr_low = c(0.7, 0.7, 0.7, 0.7, 0.7)
# not recommended to 'exaggerate' upper bounds.
# 1.5 means channel budget can increase to 150% of current level
,channel_constr_up = c(1.2, 1.5, 1.5, 1.5, 1.5)
)
Robyn <- robyn_refresh(robyn_object = robyn_object # the location of your Robyn.RData object
, dt_input = dt_simulated_weekly
, dt_holidays = dt_prophet_holidays
# refresh_steps = 4 means refresh model's rolling window will move forward 4 weeks
, refresh_steps = 13
# "auto" means the refresh function will move forward until no more data available
, refresh_mode = "manual"
, refresh_iters = 150 # iteration for refresh
, refresh_trials = 1 # trial for refresh
)
AllocatorCollect <- robyn_allocator(
robyn_object = robyn_object
# , select_run
, optim_algo = "SLSQP_AUGLAG" # "MMA_AUGLAG", "SLSQP_AUGLAG"
, scenario = "max_historical_response" # c(max_historical_response, max_response_expected_spend)
# specify future spend volume. only applies when scenario = "max_response_expected_spend"
#, expected_spend = 100000
# specify period for the future spend volumne in days. only applies when scenario = "max_response_expected_spend"
#, expected_spend_days = 90
# must be between 0.01-1 and has same length and order as paid_media_vars
, channel_constr_low = c(0.7, 0.7, 0.7, 0.7, 0.7)
# not recommended to 'exaggerate' upper bounds. 1.5 means channel budget can increase to 150% of current level
, channel_constr_up = c(1.2, 1.5, 1.5, 1.5, 1.5)
)
# get response for 80k
Spend1 <- 80000
Response1 <- robyn_response(robyn_object = robyn_object
#, select_run = 1 # 2 means the sedond refresh model. 0 means the initial model
, paid_media_var = "search_clicks_P"
, Spend = Spend1)
Response1/Spend1 # ROI for search 80k
# get response for 81k
Spend2 <- Spend1+1000
Response2 <- robyn_response(robyn_object = robyn_object
#, select_run = 1
, paid_media_var = "search_clicks_P"
, Spend = Spend2)
Response2/Spend2 # ROI for search 81k
# marginal ROI of next 1000$ from 80k spend level for search
(Response2-Response1)/(Spend2-Spend1)
devtools::document()
devtools::document()
devtools::document()
devtools::document()
detectCores()
?wday
?day
day(Sys.Date())
lubridate::day(Sys.Date())
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
?detectCores
?weekday
weekday
devtools::document()
?multicoremulticore
multicore?
?multicore
?multicore
?plan
devtools::document()
InputCollect$calibration_input
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::install_github("facebookexperimental/Robyn@package_test")
## Install and load libraries
library(Robyn) # devtools::install_github("facebookexperimental/Robyn@package_test")
## Check simulated dataset or load your own dataset
data("dt_simulated_weekly")
head(dt_simulated_weekly)
## Check holidays from Prophet
# 59 countries included. If your country is not included, please manually add it.
# Tipp: any events can be added into this table, school break, events etc.
data("dt_prophet_holidays")
head(dt_prophet_holidays)
## Set robyn_object. It must have extension .RData. The object name can be different than Robyn:
robyn_object <- "~/Desktop/Robyn.RData"
InputCollect <- robyn_inputs(
dt_input = dt_simulated_weekly
,dt_holidays = dt_prophet_holidays
#######################
#### set variables ####
,date_var = "DATE" # date format must be "2020-01-01"
,dep_var = "revenue" # there should be only one dependent variable
,dep_var_type = "revenue" # "revenue" or "conversion"
# "trend","season", "weekday", "holiday" are provided and case-sensitive.
# Recommended to at least keep Trend & Holidays
,prophet_vars = c("trend", "season", "holiday")
# c("default", "positive", and "negative"). Recommend as default.
# Must be same length as prophet_vars
,prophet_signs = c("default","default", "default")
# only one country allowed once. Including national holidays for 59 countries,
# whose list can be found on our githut guide
,prophet_country = "DE"
# typically competitors, price & promotion, temperature,  unemployment rate etc
,context_vars = c("competitor_sales_B", "events")
# c("default", "positive", and "negative"), control the signs of coefficients for baseline variables
,context_signs = c("default", "default")
# c("tv_S"	,"ooh_S",	"print_S"	,"facebook_I", "facebook_S"	,"search_clicks_P"	,"search_S").
# We recommend to use media exposure metrics like impressions, GRP etc for the model.
# If not applicable, use spend instead
,paid_media_vars = c("tv_S","ooh_S"	,	"print_S"	,"facebook_I"	,"search_clicks_P")
# c("default", "positive", and "negative"). must have same length as paid_media_vars.
# control the signs of coefficients for media variables
,paid_media_signs = c("positive", "positive","positive", "positive", "positive")
# spends must have same order and same length as paid_media_vars
,paid_media_spends = c("tv_S","ooh_S",	"print_S"	,"facebook_S"	,"search_S")
,organic_vars = c("newsletter")
,organic_signs = c("positive") # must have same length as organic_vars
,factor_vars = c("events") # specify which variables in context_vars and organic_vars are factorial
##############################
#### set model parameters ####
## set cores for parallel computing
,cores = 6 # I am using 6 cores from 8 on my local machine. Use detectCores() to find out cores
## set rolling window start
,window_start = "2016-11-23"
,window_end = "2018-08-22"
## set model core features
# Weibull is more flexible, yet has one more parameter and thus takes longer
,adstock = "geometric" # geometric or weibull
,iterations = 100  # number of allowed iterations per trial. 500 is recommended
# selected algorithm for Nevergrad, the gradient-free optimization library
# https://facebookresearch.github.io/nevergrad/index.html
,nevergrad_algo = "TwoPointsDE"
# 40 is recommended without calibration, 100 with calibration.
,trials = 2 # number of allowed iterations per trial
## Time estimation: with geometric adstock, 2000 iterations * 5 trials and 6 cores,
# it takes about 30 minutes. Weibull takes at least twice as much time.
#,hyperparameters = hyperparameters
# ,calibration_input = data.table(channel = c("facebook_I",  "tv_S", "facebook_I"),
#                        liftStartDate = as.Date(c("2018-05-01", "2017-11-27", "2018-07-01")),
#                        liftEndDate = as.Date(c("2018-06-10", "2017-12-03", "2018-07-20")),
#                        liftAbs = c(400000, 300000, 200000))
)
# helper plots: set plot to TRUE for transformation examples:
# adstock transformation example plot, helping you understand geometric/theta and weibull/shape/scale transformation
plot_adstock(FALSE)
# s-curve transformation example plot, helping you understand hill/alpha/gamma transformatio
plot_saturation(FALSE)
hyper_names(adstock = InputCollect$adstock, all_media = InputCollect$all_media)
hyperparameters <- list(
facebook_I_alphas = c(0.5, 3) # example bounds for alpha
,facebook_I_gammas = c(0.3, 1) # example bounds for gamma
,facebook_I_thetas = c(0, 0.3) # example bounds for theta
#,facebook_I_shapes = c(0.0001, 2) # example bounds for shape
#,facebook_I_scales = c(0, 0.1) # example bounds for scale
,print_S_alphas = c(0.5, 3)
,print_S_gammas = c(0.3, 1)
,print_S_thetas = c(0.1, 0.4)
#,print_S_shapes = c(0.0001, 2)
#,print_S_scales = c(0, 0.1)
,tv_S_alphas = c(0.5, 3)
,tv_S_gammas = c(0.3, 1)
,tv_S_thetas = c(0.3, 0.8)
#,tv_S_shapes = c(0.0001, 2)
#,tv_S_scales= c(0, 0.1)
,search_clicks_P_alphas = c(0.5, 3)
,search_clicks_P_gammas = c(0.3, 1)
,search_clicks_P_thetas = c(0, 0.3)
#,search_clicks_P_shapes = c(0.0001, 2)
#,search_clicks_P_scales = c(0, 0.1)
,ooh_S_alphas = c(0.5, 3)
,ooh_S_gammas = c(0.3, 1)
,ooh_S_thetas = c(0.1, 0.4)
#,ooh_S_shapes = c(0.0001, 2)
#,ooh_S_scales = c(0, 0.1)
,newsletter_alphas = c(0.5, 3)
,newsletter_gammas = c(0.3, 1)
,newsletter_thetas = c(0.1, 0.4)
#,newsletter_shapes = c(0.0001, 2)
#,newsletter_scales = c(0, 0.1)
)
InputCollect <- robyn_inputs(InputCollect = InputCollect
, hyperparameters = hyperparameters)
devtools::document()
devtools::document()
devtools::document()
devtools::document()
OutputCollect <- robyn_run(InputCollect = InputCollect
, plot_folder = robyn_object
, pareto_fronts = 1
, plot_pareto = TRUE)
