?InputCollect
devtools::document()
ng <<- reticulate::import("nevergrad", delay_load = TRUE)
ng
devtools::document()
askYesNo
devtools::document()
percentage()
percentage
?season
devtools::document()
devtools::document()
?nrmse
??nrmse
?robyn_inputs
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
reticulate::py_install("matplotlib")
system("conda update -n base -c defaults conda")
devtools::document()
library(Robyn)
## Check simulatetd dataset or load your own dataset
data("dt_simulated_weekly")
head(dt_simulated_weekly)
## Check holidays from Prophet
# 59 countries included. If your country is not included, please manually add it.
# Tipp: any events can be added into this table, school break, events etc.
data("dt_prophet_holidays")
head(dt_prophet_holidays)
## Set robyn_object. It must has extension .RData. The object name can be different than Robyn
robyn_object <- "~/Desktop/Robyn.RData"
InputCollect <- robyn_inputs(
dt_input = dt_simulated_weekly
,dt_holidays = dt_prophet_holidays
#######################
#### set variables ####
,date_var = "DATE" # date format must be "2020-01-01"
,dep_var = "revenue" # there should be only one dependent variable
,dep_var_type = "revenue" # "revenue" or "conversion"
# "trend","season", "weekday", "holiday" are provided and case-sensitive.
# Recommended to at least keep Trend & Holidays
,prophet_vars = c("trend", "season", "holiday")
# c("default", "positive", and "negative"). Recommend as default.
# Must be same length as prophet_vars
,prophet_signs = c("default","default", "default")
# only one country allowed once. Including national holidays for 59 countries,
# whose list can be found on our githut guide
,prophet_country = "DE"
# typically competitors, price & promotion, temperature,  unemployment rate etc
,context_vars = c("competitor_sales_B", "events")
# c("default", "positive", and "negative"), control the signs of coefficients for baseline variables
,context_signs = c("default", "default")
# c("tv_S"	,"ooh_S",	"print_S"	,"facebook_I", "facebook_S"	,"search_clicks_P"	,"search_S").
# We recommend to use media exposure metrics like impressions, GRP etc for the model.
# If not applicable, use spend instead
,paid_media_vars = c("tv_S","ooh_S"	,	"print_S"	,"facebook_I"	,"search_clicks_P")
# c("default", "positive", and "negative"). must have same length as paid_media_vars.
# control the signs of coefficients for media variables
,paid_media_signs = c("positive", "positive","positive", "positive", "positive")
# spends must have same order and same length as paid_media_vars
,paid_media_spends = c("tv_S","ooh_S",	"print_S"	,"facebook_S"	,"search_S")
,organic_vars = c("newsletter")
,organic_signs = c("positive") # must have same length as organic_vars
,factor_vars = c("events") # specify which variables in context_vars and organic_vars are factorial
##############################
#### set model parameters ####
## set cores for parallel computing
,cores = 6 # I am using 6 cores from 8 on my local machine. Use detectCores() to find out cores
## set rolling window start
,window_start = "2016-11-23"
,window_end = "2018-08-22"
## set model core features
# Weibull is more flexible, yet has one more parameter and thus takes longer
,adstock = "geometric" # geometric or weibull
,iterations = 100  # number of allowed iterations per trial. 500 is recommended
# selected algorithm for Nevergrad, the gradient-free optimisation library
# https://facebookresearch.github.io/nevergrad/index.html
,nevergrad_algo = "TwoPointsDE"
# 40 is recommended without calibration, 100 with calibration.
,trials = 2 # number of allowed iterations per trial
## Time estimation: with geometric adstock, 2000 iterations * 5 trials and 6 cores,
# it takes about 30 minutes. Weibull takes at least twice as much time.
#,hyperparameters = hyperparameters
# ,calibration_input = data.table(channel = c("facebook_I",  "tv_S", "facebook_I"),
#                        liftStartDate = as.Date(c("2018-05-01", "2017-11-27", "2018-07-01")),
#                        liftEndDate = as.Date(c("2018-06-10", "2017-12-03", "2018-07-20")),
#                        liftAbs = c(400000, 300000, 200000))
)
library(Robyn)
library(Robyn)
InputCollect <- robyn_inputs(
dt_input = dt_simulated_weekly
,dt_holidays = dt_prophet_holidays
#######################
#### set variables ####
,date_var = "DATE" # date format must be "2020-01-01"
,dep_var = "revenue" # there should be only one dependent variable
,dep_var_type = "revenue" # "revenue" or "conversion"
# "trend","season", "weekday", "holiday" are provided and case-sensitive.
# Recommended to at least keep Trend & Holidays
,prophet_vars = c("trend", "season", "holiday")
# c("default", "positive", and "negative"). Recommend as default.
# Must be same length as prophet_vars
,prophet_signs = c("default","default", "default")
# only one country allowed once. Including national holidays for 59 countries,
# whose list can be found on our githut guide
,prophet_country = "DE"
# typically competitors, price & promotion, temperature,  unemployment rate etc
,context_vars = c("competitor_sales_B", "events")
# c("default", "positive", and "negative"), control the signs of coefficients for baseline variables
,context_signs = c("default", "default")
# c("tv_S"	,"ooh_S",	"print_S"	,"facebook_I", "facebook_S"	,"search_clicks_P"	,"search_S").
# We recommend to use media exposure metrics like impressions, GRP etc for the model.
# If not applicable, use spend instead
,paid_media_vars = c("tv_S","ooh_S"	,	"print_S"	,"facebook_I"	,"search_clicks_P")
# c("default", "positive", and "negative"). must have same length as paid_media_vars.
# control the signs of coefficients for media variables
,paid_media_signs = c("positive", "positive","positive", "positive", "positive")
# spends must have same order and same length as paid_media_vars
,paid_media_spends = c("tv_S","ooh_S",	"print_S"	,"facebook_S"	,"search_S")
,organic_vars = c("newsletter")
,organic_signs = c("positive") # must have same length as organic_vars
,factor_vars = c("events") # specify which variables in context_vars and organic_vars are factorial
##############################
#### set model parameters ####
## set cores for parallel computing
,cores = 6 # I am using 6 cores from 8 on my local machine. Use detectCores() to find out cores
## set rolling window start
,window_start = "2016-11-23"
,window_end = "2018-08-22"
## set model core features
# Weibull is more flexible, yet has one more parameter and thus takes longer
,adstock = "geometric" # geometric or weibull
,iterations = 100  # number of allowed iterations per trial. 500 is recommended
# selected algorithm for Nevergrad, the gradient-free optimisation library
# https://facebookresearch.github.io/nevergrad/index.html
,nevergrad_algo = "TwoPointsDE"
# 40 is recommended without calibration, 100 with calibration.
,trials = 2 # number of allowed iterations per trial
## Time estimation: with geometric adstock, 2000 iterations * 5 trials and 6 cores,
# it takes about 30 minutes. Weibull takes at least twice as much time.
#,hyperparameters = hyperparameters
# ,calibration_input = data.table(channel = c("facebook_I",  "tv_S", "facebook_I"),
#                        liftStartDate = as.Date(c("2018-05-01", "2017-11-27", "2018-07-01")),
#                        liftEndDate = as.Date(c("2018-06-10", "2017-12-03", "2018-07-20")),
#                        liftAbs = c(400000, 300000, 200000))
)
# helper plots: set plot to TRUE for transformation examples:
# adstock transformation example plot, helping you understand geometric/theta and weibull/shape/scale transformation
plot_adstock(FALSE)
# s-curve transformation example plot, helping you understand hill/alpha/gamma transformatio
plot_saturation(FALSE)
hyper_names(adstock = InputCollect$adstock, all_media = InputCollect$all_media)
hyperparameters <- list(
facebook_I_alphas = c(0.5, 3) # example bounds for alpha
,facebook_I_gammas = c(0.3, 1) # example bounds for gamma
,facebook_I_thetas = c(0, 0.3) # example bounds for theta
#,facebook_I_shapes = c(0.0001, 2) # example bounds for shape
#,facebook_I_scales = c(0, 0.1) # example bounds for scale
,print_S_alphas = c(0.5, 3)
,print_S_gammas = c(0.3, 1)
,print_S_thetas = c(0.1, 0.4)
#,print_S_shapes = c(0.0001, 2)
#,print_S_scales = c(0, 0.1)
,tv_S_alphas = c(0.5, 3)
,tv_S_gammas = c(0.3, 1)
,tv_S_thetas = c(0.3, 0.8)
#,tv_S_shapes = c(0.0001, 2)
#,tv_S_scales= c(0, 0.1)
,search_clicks_P_alphas = c(0.5, 3)
,search_clicks_P_gammas = c(0.3, 1)
,search_clicks_P_thetas = c(0, 0.3)
#,search_clicks_P_shapes = c(0.0001, 2)
#,search_clicks_P_scales = c(0, 0.1)
,ooh_S_alphas = c(0.5, 3)
,ooh_S_gammas = c(0.3, 1)
,ooh_S_thetas = c(0.1, 0.4)
#,ooh_S_shapes = c(0.0001, 2)
#,ooh_S_scales = c(0, 0.1)
,newsletter_alphas = c(0.5, 3)
,newsletter_gammas = c(0.3, 1)
,newsletter_thetas = c(0.1, 0.4)
#,newsletter_shapes = c(0.0001, 2)
#,newsletter_scales = c(0, 0.1)
)
InputCollect <- robyn_inputs(InputCollect = InputCollect
, hyperparameters = hyperparameters)
?add_regressor
devtools::document()
devtools::document()
library(Robyn)
InputCollect <- robyn_inputs(InputCollect = InputCollect
, hyperparameters = hyperparameters)
OutputCollect <- robyn_run(InputCollect = InputCollect
, plot_folder = robyn_object
, pareto_fronts = 1
, plot_pareto = TRUE)
?registerDoFuture
??registerDoFuture
library(Robyn)
OutputCollect <- robyn_run(InputCollect = InputCollect
, plot_folder = robyn_object
, pareto_fronts = 1
, plot_pareto = TRUE)
