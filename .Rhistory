,prophet_vars = c("trend", "season", "holiday") # "trend","season", "weekday", "holiday"
# are provided and case-sensitive. Recommended to at least keep Trend & Holidays
,prophet_signs = c("default","default", "default") # c("default", "positive", and "negative").
# Recommend as default.Must be same length as prophet_vars
,prophet_country = "DE"# only one country allowed once. Including national holidays
# for 59 countries, whose list can be found on our githut guide
,context_vars = c("competitor_sales_B", "events") # typically competitors, price &
# promotion, temperature, unemployment rate etc
,context_signs = c("default", "default") # c("default", " positive", and "negative"),
# control the signs of coefficients for baseline variables
,paid_media_vars = c("tv_S", "ooh_S"	,	"print_S"	,"facebook_I" ,"search_clicks_P")
# c("tv_S"	,"ooh_S",	"print_S"	,"facebook_I", "facebook_S","search_clicks_P"	,"search_S")
# we recommend to use media exposure metrics like impressions, GRP etc for the model.
# If not applicable, use spend instead
,paid_media_signs = c("positive", "positive","positive", "positive", "positive")
# c("default", "positive", and "negative"). must have same length as paid_media_vars.
# Controls the signs of coefficients for media variables
,paid_media_spends = c("tv_S","ooh_S",	"print_S"	,"facebook_S", "search_S")
# spends must have same order and same length as paid_media_vars
,organic_vars = c("newsletter")
,organic_signs = c("positive") # must have same length as organic_vars
,factor_vars = c("events") # specify which variables in context_vars and
# organic_vars are factorial
### set model parameters
## set cores for parallel computing
,cores = 6 # I am using 6 cores from 8 on my local machine. Use availableCores() to find out cores
## set rolling window start
,window_start = "2016-11-23"
,window_end = "2018-08-22"
## set model core features
,adstock = "geometric" # geometric or weibull. weibull is more flexible, yet has one more
# parameter and thus takes longer
,iterations = 100  # number of allowed iterations per trial. 2000 is recommended
,nevergrad_algo = "TwoPointsDE" # recommended algorithm for Nevergrad, the gradient-free
# optimisation library https://facebookresearch.github.io/nevergrad/index.html
,trials = 2 # number of allowed iterations per trial. 5 is recommended without calibration,
# 10 with calibration.
# Time estimation: with geometric adstock, 2000 iterations * 5 trials
# and 6 cores, it takes less than 1 hour. Weibull takes at least twice as much time.
)
InputCollect <- robyn_inputs(
dt_input = dt_simulated_weekly
,dt_holidays = dt_prophet_holidays
### set variables
,date_var = "DATE" # date format must be "2020-01-01"
,dep_var = "revenue" # there should be only one dependent variable
,dep_var_type = "revenue" # "revenue" or "conversion"
,prophet_vars = c("trend", "season", "holiday") # "trend","season", "weekday", "holiday"
# are provided and case-sensitive. Recommended to at least keep Trend & Holidays
,prophet_signs = c("default","default", "default") # c("default", "positive", and "negative").
# Recommend as default.Must be same length as prophet_vars
,prophet_country = "DE"# only one country allowed once. Including national holidays
# for 59 countries, whose list can be found on our githut guide
,context_vars = c("competitor_sales_B", "events") # typically competitors, price &
# promotion, temperature, unemployment rate etc
,context_signs = c("default", "default") # c("default", " positive", and "negative"),
# control the signs of coefficients for baseline variables
,paid_media_vars = c("tv_S", "ooh_S"	,	"print_S"	,"facebook_I" ,"search_clicks_P")
# c("tv_S"	,"ooh_S",	"print_S"	,"facebook_I", "facebook_S","search_clicks_P"	,"search_S")
# we recommend to use media exposure metrics like impressions, GRP etc for the model.
# If not applicable, use spend instead
,paid_media_signs = c("positive", "positive","positive", "positive", "positive")
# c("default", "positive", and "negative"). must have same length as paid_media_vars.
# Controls the signs of coefficients for media variables
,paid_media_spends = c("tv_S","ooh_S",	"print_S"	,"facebook_S", "search_S")
# spends must have same order and same length as paid_media_vars
,organic_vars = c("newsletter")
,organic_signs = c("positive") # must have same length as organic_vars
,factor_vars = c("events") # specify which variables in context_vars and
# organic_vars are factorial
### set model parameters
## set cores for parallel computing
,cores = 6 # I am using 6 cores from 8 on my local machine. Use availableCores() to find out cores
## set rolling window start
,window_start = "2016-11-23"
,window_end = "2018-08-22"
## set model core features
,adstock = "geometric" # geometric or weibull. weibull is more flexible, yet has one more
# parameter and thus takes longer
,iterations = 100  # number of allowed iterations per trial. 2000 is recommended
,nevergrad_algo = "TwoPointsDE" # recommended algorithm for Nevergrad, the gradient-free
# optimisation library https://facebookresearch.github.io/nevergrad/index.html
,trials = 2 # number of allowed iterations per trial. 5 is recommended without calibration,
# 10 with calibration.
# Time estimation: with geometric adstock, 2000 iterations * 5 trials
# and 6 cores, it takes less than 1 hour. Weibull takes at least twice as much time.
)
# helper plots: set plot to TRUE for transformation examples
plot_adstock(FALSE) # adstock transformation example plot,
# helping you understand geometric/theta and weibull/shape/scale transformation
plot_saturation(FALSE) # s-curve transformation example plot,
hyper_names(adstock = InputCollect$adstock, all_media = InputCollect$all_media)
hyperparameters <- list(
facebook_I_alphas = c(0.5, 3) # example bounds for alpha
,facebook_I_gammas = c(0.3, 1) # example bounds for gamma
,facebook_I_thetas = c(0, 0.3) # example bounds for theta
#,facebook_I_shapes = c(0.0001, 2) # example bounds for shape
#,facebook_I_scales = c(0, 0.1) # example bounds for scale
,print_S_alphas = c(0.5, 3)
,print_S_gammas = c(0.3, 1)
,print_S_thetas = c(0.1, 0.4)
#,print_S_shapes = c(0.0001, 2)
#,print_S_scales = c(0, 0.1)
,tv_S_alphas = c(0.5, 3)
,tv_S_gammas = c(0.3, 1)
,tv_S_thetas = c(0.3, 0.8)
#,tv_S_shapes = c(0.0001, 2)
#,tv_S_scales= c(0, 0.1)
,search_clicks_P_alphas = c(0.5, 3)
,search_clicks_P_gammas = c(0.3, 1)
,search_clicks_P_thetas = c(0, 0.3)
#,search_clicks_P_shapes = c(0.0001, 2)
#,search_clicks_P_scales = c(0, 0.1)
,ooh_S_alphas = c(0.5, 3)
,ooh_S_gammas = c(0.3, 1)
,ooh_S_thetas = c(0.1, 0.4)
#,ooh_S_shapes = c(0.0001, 2)
#,ooh_S_scales = c(0, 0.1)
,newsletter_alphas = c(0.5, 3)
,newsletter_gammas = c(0.3, 1)
,newsletter_thetas = c(0.1, 0.4)
#,newsletter_shapes = c(0.0001, 2)
#,newsletter_scales = c(0, 0.1)
)
InputCollect <- robyn_inputs(InputCollect = InputCollect, hyperparameters = hyperparameters)
OutputCollect <- robyn_run(
InputCollect = InputCollect # feed in all model specification
, plot_folder = robyn_object # plots will be saved in the same folder as robyn_object.
# Other paths are also possible
, pareto_fronts = 1 # How many Pareto fronts to be exported as model output. The higher,
# the more model results/ one-pagers provided
, plot_pareto = TRUE # Disabling plat_pareto speeds up processing time in the end,
# but plots won't be saved
)
?prophet
library(Robyn)
?robyn_inputs
library(Robyn)
?robyn_inputs
library(Robyn)
?robyn_inputs
library(Robyn)
?robyn_inputs
library(Robyn)
?robyn_inputs
library(Robyn)
?robyn_inputs
?robyn_inputs
?hyper_names
library(Robyn)
?hyper_names
library(Robyn)
?hyper_names
library(Robyn)
summary.lm
## Install and load libraries
library(Robyn) # devtools::install_github("facebookexperimental/Robyn@package_test")
set.seed(123)
## force multicore when using RStudio
Sys.setenv(R_FUTURE_FORK_ENABLE="true")
options(future.fork.enable = TRUE)
## Check simulated dataset or load your own dataset
data("dt_simulated_weekly")
head(dt_simulated_weekly)
## Check holidays from Prophet
# 59 countries included. If your country is not included, please manually add it.
# Tipp: any events can be added into this table, school break, events etc.
data("dt_prophet_holidays")
head(dt_prophet_holidays)
## Set robyn_object. It must have extension .RData. The object name can be different than Robyn:
robyn_object <- "~/Desktop/Robyn.RData"
InputCollect <- robyn_inputs(
dt_input = dt_simulated_weekly
,dt_holidays = dt_prophet_holidays
### set variables
,date_var = "DATE" # date format must be "2020-01-01"
,dep_var = "revenue" # there should be only one dependent variable
,dep_var_type = "revenue" # "revenue" or "conversion"
,prophet_vars = c("trend", "season", "holiday") # "trend","season", "weekday", "holiday"
# are provided and case-sensitive. Recommended to at least keep Trend & Holidays
,prophet_signs = c("default","default", "default") # c("default", "positive", and "negative").
# Recommend as default.Must be same length as prophet_vars
,prophet_country = "DE"# only one country allowed once. Including national holidays
# for 59 countries, whose list can be found on our githut guide
,context_vars = c("competitor_sales_B", "events") # typically competitors, price &
# promotion, temperature, unemployment rate etc
,context_signs = c("default", "default") # c("default", " positive", and "negative"),
# control the signs of coefficients for baseline variables
,paid_media_vars = c("tv_S", "ooh_S"	,	"print_S"	,"facebook_I" ,"search_clicks_P")
# c("tv_S"	,"ooh_S",	"print_S"	,"facebook_I", "facebook_S","search_clicks_P"	,"search_S")
# we recommend to use media exposure metrics like impressions, GRP etc for the model.
# If not applicable, use spend instead
,paid_media_signs = c("positive", "positive","positive", "positive", "positive")
# c("default", "positive", and "negative"). must have same length as paid_media_vars.
# Controls the signs of coefficients for media variables
,paid_media_spends = c("tv_S","ooh_S",	"print_S"	,"facebook_S", "search_S")
# spends must have same order and same length as paid_media_vars
,organic_vars = c("newsletter")
,organic_signs = c("positive") # must have same length as organic_vars
,factor_vars = c("events") # specify which variables in context_vars and
# organic_vars are factorial
### set model parameters
## set cores for parallel computing
,cores = 6 # I am using 6 cores from 8 on my local machine. Use availableCores() to find out cores
## set rolling window start
,window_start = "2016-11-23"
,window_end = "2018-08-22"
## set model core features
,adstock = "geometric" # geometric or weibull. weibull is more flexible, yet has one more
# parameter and thus takes longer
,iterations = 100  # number of allowed iterations per trial. 2000 is recommended
,nevergrad_algo = "TwoPointsDE" # recommended algorithm for Nevergrad, the gradient-free
# optimisation library https://facebookresearch.github.io/nevergrad/index.html
,trials = 2 # number of allowed iterations per trial. 5 is recommended without calibration,
# 10 with calibration.
# Time estimation: with geometric adstock, 2000 iterations * 5 trials
# and 6 cores, it takes less than 1 hour. Weibull takes at least twice as much time.
)
hyper_names(adstock = InputCollect$adstock, all_media = InputCollect$all_media)
# helper plots: set plot to TRUE for transformation examples
plot_adstock(T) # adstock transformation example plot,
# helping you understand geometric/theta and weibull/shape/scale transformation
plot_saturation(T) # s-curve transformation example plot,
InputCollect$adstock
InputCollect$all_media
hyper_names(adstock = InputCollect$adstock, all_media = InputCollect$all_media)
hyperparameters <- list(
facebook_I_alphas = c(0.5, 3) # example bounds for alpha
,facebook_I_gammas = c(0.3, 1) # example bounds for gamma
,facebook_I_thetas = c(0, 0.3) # example bounds for theta
#,facebook_I_shapes = c(0.0001, 2) # example bounds for shape
#,facebook_I_scales = c(0, 0.1) # example bounds for scale
,print_S_alphas = c(0.5, 3)
,print_S_gammas = c(0.3, 1)
,print_S_thetas = c(0.1, 0.4)
#,print_S_shapes = c(0.0001, 2)
#,print_S_scales = c(0, 0.1)
,tv_S_alphas = c(0.5, 3)
,tv_S_gammas = c(0.3, 1)
,tv_S_thetas = c(0.3, 0.8)
#,tv_S_shapes = c(0.0001, 2)
#,tv_S_scales= c(0, 0.1)
,search_clicks_P_alphas = c(0.5, 3)
,search_clicks_P_gammas = c(0.3, 1)
,search_clicks_P_thetas = c(0, 0.3)
#,search_clicks_P_shapes = c(0.0001, 2)
#,search_clicks_P_scales = c(0, 0.1)
,ooh_S_alphas = c(0.5, 3)
,ooh_S_gammas = c(0.3, 1)
,ooh_S_thetas = c(0.1, 0.4)
#,ooh_S_shapes = c(0.0001, 2)
#,ooh_S_scales = c(0, 0.1)
,newsletter_alphas = c(0.5, 3)
,newsletter_gammas = c(0.3, 1)
,newsletter_thetas = c(0.1, 0.4)
#,newsletter_shapes = c(0.0001, 2)
#,newsletter_scales = c(0, 0.1)
)
InputCollect <- robyn_inputs(InputCollect = InputCollect, hyperparameters = hyperparameters)
InputCollect <- robyn_inputs(
dt_input = dt_simulated_weekly
,dt_holidays = dt_prophet_holidays
### set variables
,date_var = "DATE" # date format must be "2020-01-01"
,dep_var = "revenue" # there should be only one dependent variable
,dep_var_type = "revenue" # "revenue" or "conversion"
,prophet_vars = c("trend", "season", "holiday") # "trend","season", "weekday", "holiday"
# are provided and case-sensitive. Recommended to at least keep Trend & Holidays
,prophet_signs = c("default","default", "default") # c("default", "positive", and "negative").
# Recommend as default.Must be same length as prophet_vars
,prophet_country = "DE"# only one country allowed once. Including national holidays
# for 59 countries, whose list can be found on our githut guide
,context_vars = c("competitor_sales_B", "events") # typically competitors, price &
# promotion, temperature, unemployment rate etc
,context_signs = c("default", "default") # c("default", " positive", and "negative"),
# control the signs of coefficients for baseline variables
,paid_media_vars = c("tv_S", "ooh_S"	,	"print_S"	,"facebook_I" ,"search_clicks_P")
# c("tv_S"	,"ooh_S",	"print_S"	,"facebook_I", "facebook_S","search_clicks_P"	,"search_S")
# we recommend to use media exposure metrics like impressions, GRP etc for the model.
# If not applicable, use spend instead
,paid_media_signs = c("positive", "positive","positive", "positive", "positive")
# c("default", "positive", and "negative"). must have same length as paid_media_vars.
# Controls the signs of coefficients for media variables
,paid_media_spends = c("tv_S","ooh_S",	"print_S"	,"facebook_S", "search_S")
# spends must have same order and same length as paid_media_vars
,organic_vars = c("newsletter")
,organic_signs = c("positive") # must have same length as organic_vars
,factor_vars = c("events") # specify which variables in context_vars and
# organic_vars are factorial
### set model parameters
## set cores for parallel computing
,cores = 6 # I am using 6 cores from 8 on my local machine. Use availableCores() to find out cores
## set rolling window start
,window_start = "2016-11-23"
,window_end = "2018-08-22"
## set model core features
,adstock = "geometric" # geometric or weibull. weibull is more flexible, yet has one more
# parameter and thus takes longer
,iterations = 100  # number of allowed iterations per trial. 2000 is recommended
,nevergrad_algo = "TwoPointsDE" # recommended algorithm for Nevergrad, the gradient-free
# optimisation library https://facebookresearch.github.io/nevergrad/index.html
,trials = 2 # number of allowed iterations per trial. 5 is recommended without calibration,
# 10 with calibration.
# Time estimation: with geometric adstock, 2000 iterations * 5 trials
# and 6 cores, it takes less than 1 hour. Weibull takes at least twice as much time.
)
InputCollect <- robyn_inputs(InputCollect = InputCollect, hyperparameters = hyperparameters)
?hyper_names
library(Robyn)
?hyper_names
## clear env
# .rs.restartR()
rm(list=ls())
## clear env
# .rs.restartR()
rm(list=ls())
## load libs
load_libs <- function() {
libsNeeded <- c("devtools", "roxygen2","data.table","stringr","lubridate","foreach","glmnet","car","prophet","ggplot2","nloptr","minpack.lm","rPref","reticulate","rstudioapi","doFuture", "doRNG", "patchwork"
)
cat(paste0("Loading required libraries: ", paste(libsNeeded, collapse = ", "), "\n"))
libsInstalled <- rownames(installed.packages())
for (l in libsNeeded) {
if (require(l, character.only = TRUE)) {} else {
install.packages(l)
require(l, character.only = TRUE)
}
}
}
load_libs()
## load scripts
script_path = substr(rstudioapi::getActiveDocumentContext()$path, start = 1, stop = max(gregexpr("/", rstudioapi::getActiveDocumentContext()$path)[[1]]))
script_path <- sub("/inst/", "/R/", script_path)
source(paste(script_path, "allocator.R", sep=""))
source(paste(script_path, "auxiliary.R", sep=""))
source(paste(script_path, "checks.R", sep=""))
source(paste(script_path, "inputs.R", sep=""))
source(paste(script_path, "model.R", sep=""))
source(paste(script_path, "refresh.R", sep=""))
source(paste(script_path, "transformation.R", sep=""))
data_path <- sub("/R/", "/data/", script_path)
load(paste0(data_path, "dt_prophet_holidays.RData"))
load(paste0(data_path, "dt_simulated_weekly.RData"))
## get input for robyn_run() debugging
if (T) {
dt_input = dt_simulated_weekly
dt_holidays = dt_prophet_holidays
date_var = "DATE" # date format must be "2020-01-01"
dep_var = "revenue" # there should be only one dependent variable
dep_var_type = "revenue" # "revenue" or "conversion"
prophet_vars = c("trend", "season", "holiday") # "trend","season", "weekday", "holiday"
prophet_signs = c("default","default", "default") # c("default", "positive", and "negative").
prophet_country = "DE"# only one country allowed once. Including national holidays
context_vars = c("competitor_sales_B", "events") # typically competitors, price &
context_signs = c("default", "default") # c("default", " positive", and "negative"),
paid_media_vars = c("tv_S", "ooh_S"	,	"print_S"	,"facebook_I" ,"search_clicks_P")
paid_media_signs = c("positive", "positive","positive", "positive", "positive")
paid_media_spends = c("tv_S","ooh_S",	"print_S"	,"facebook_S", "search_S")
organic_vars = c("newsletter")
organic_signs = c("positive") # must have same length as organic_vars
factor_vars = c("events") # specify which variables in context_vars and
cores = 6 # I am using 6 cores from 8 on my local machine. Use availableCores() to find out cores
window_start = "2016-11-23"
window_end = "2018-08-22"
adstock = "geometric" # geometric or weibull. weibull is more flexible, yet has one more
iterations = 100  # number of allowed iterations per trial. 2000 is recommended
nevergrad_algo = "TwoPointsDE" # recommended algorithm for Nevergrad, the gradient-free
trials = 2 # number of allowed iterations per trial. 5 is recommended without calibration,
calibration_input = NULL
dt_calibration <- data.frame(
channel = c("facebook_I",  "tv_S", "facebook_I")
# channel name must in paid_media_vars
, liftStartDate = as.Date(c("2018-05-01", "2017-11-27", "2018-07-01"))
# liftStartDate must be within input data range
, liftEndDate = as.Date(c("2018-06-10", "2017-12-03", "2018-07-20"))
# liftEndDate must be within input data range
, liftAbs = c(400000, 300000, 200000) # Provided value must be
# tested on same campaign level in model and same metric as dep_var_type
)
}
dt_input <- as.data.table(dt_input)
dt_holidays <- as.data.table(dt_holidays)
## check date input (and set dayInterval and intervalType)
date_input <- check_datevar(dt_input, date_var)
date_var <- date_input$date_var # when date_var = "auto"
dayInterval <- date_input$dayInterval
intervalType <- date_input$intervalType
## check dependent var
check_depvar(dt_input, dep_var, dep_var_type)
## check prophet
check_prophet(dt_holidays, prophet_country, prophet_vars, prophet_signs)
## check baseline variables (and maybe transform context_signs)
context <- check_context(dt_input, context_vars, context_signs)
context_signs <- context$context_signs
## check paid media variables (set mediaVarCount and maybe transform paid_media_signs)
paidmedia <- check_paidmedia(dt_input, paid_media_vars, paid_media_signs, paid_media_spends)
paid_media_signs <- paidmedia$paid_media_signs
mediaVarCount <- paidmedia$mediaVarCount
exposureVarName <- paid_media_vars[!(paid_media_vars == paid_media_spends)]
library(Robyn)
## Install and load libraries
library(Robyn) # devtools::install_github("facebookexperimental/Robyn@package_test")
set.seed(123)
## force multicore when using RStudio
Sys.setenv(R_FUTURE_FORK_ENABLE="true")
options(future.fork.enable = TRUE)
## Check simulated dataset or load your own dataset
data("dt_simulated_weekly")
head(dt_simulated_weekly)
## Check holidays from Prophet
# 59 countries included. If your country is not included, please manually add it.
# Tipp: any events can be added into this table, school break, events etc.
data("dt_prophet_holidays")
head(dt_prophet_holidays)
## Set robyn_object. It must have extension .RData. The object name can be different than Robyn:
robyn_object <- "~/Desktop/Robyn.RData"
InputCollect <- robyn_inputs(
dt_input = dt_simulated_weekly
,dt_holidays = dt_prophet_holidays
### set variables
,date_var = "DATE" # date format must be "2020-01-01"
,dep_var = "revenue" # there should be only one dependent variable
,dep_var_type = "revenue" # "revenue" or "conversion"
,prophet_vars = c("trend", "season", "holiday") # "trend","season", "weekday", "holiday"
# are provided and case-sensitive. Recommended to at least keep Trend & Holidays
,prophet_signs = c("default","default", "default") # c("default", "positive", and "negative").
# Recommend as default.Must be same length as prophet_vars
,prophet_country = "DE"# only one country allowed once. Including national holidays
# for 59 countries, whose list can be found on our githut guide
,context_vars = c("competitor_sales_B", "events") # typically competitors, price &
# promotion, temperature, unemployment rate etc
,context_signs = c("default", "default") # c("default", " positive", and "negative"),
# control the signs of coefficients for baseline variables
,paid_media_vars = c("tv_S", "ooh_S"	,	"print_S"	,"facebook_I" ,"search_clicks_P")
# c("tv_S"	,"ooh_S",	"print_S"	,"facebook_I", "facebook_S","search_clicks_P"	,"search_S")
# we recommend to use media exposure metrics like impressions, GRP etc for the model.
# If not applicable, use spend instead
,paid_media_signs = c("positive", "positive","positive", "positive", "positive")
# c("default", "positive", and "negative"). must have same length as paid_media_vars.
# Controls the signs of coefficients for media variables
,paid_media_spends = c("tv_S","ooh_S",	"print_S"	,"facebook_S", "search_S")
# spends must have same order and same length as paid_media_vars
,organic_vars = c("newsletter")
,organic_signs = c("positive") # must have same length as organic_vars
,factor_vars = c("events") # specify which variables in context_vars and
# organic_vars are factorial
### set model parameters
## set cores for parallel computing
,cores = 6 # I am using 6 cores from 8 on my local machine. Use availableCores() to find out cores
## set rolling window start
,window_start = "2016-11-23"
,window_end = "2018-08-22"
## set model core features
,adstock = "geometric" # geometric or weibull. weibull is more flexible, yet has one more
# parameter and thus takes longer
,iterations = 100  # number of allowed iterations per trial. 2000 is recommended
,nevergrad_algo = "TwoPointsDE" # recommended algorithm for Nevergrad, the gradient-free
# optimisation library https://facebookresearch.github.io/nevergrad/index.html
,trials = 2 # number of allowed iterations per trial. 5 is recommended without calibration,
# 10 with calibration.
# Time estimation: with geometric adstock, 2000 iterations * 5 trials
# and 6 cores, it takes less than 1 hour. Weibull takes at least twice as much time.
)
# helper plots: set plot to TRUE for transformation examples
plot_adstock(FALSE) # adstock transformation example plot,
# helping you understand geometric/theta and weibull/shape/scale transformation
plot_saturation(FALSE) # s-curve transformation example plot,
hyper_names(adstock = InputCollect$adstock, all_media = InputCollect$all_media)
hyperparameters <- list(
facebook_I_alphas = c(0.5, 3) # example bounds for alpha
,facebook_I_gammas = c(0.3, 1) # example bounds for gamma
,facebook_I_thetas = c(0, 0.3) # example bounds for theta
#,facebook_I_shapes = c(0.0001, 2) # example bounds for shape
#,facebook_I_scales = c(0, 0.1) # example bounds for scale
,print_S_alphas = c(0.5, 3)
,print_S_gammas = c(0.3, 1)
,print_S_thetas = c(0.1, 0.4)
#,print_S_shapes = c(0.0001, 2)
#,print_S_scales = c(0, 0.1)
,tv_S_alphas = c(0.5, 3)
,tv_S_gammas = c(0.3, 1)
,tv_S_thetas = c(0.3, 0.8)
#,tv_S_shapes = c(0.0001, 2)
#,tv_S_scales= c(0, 0.1)
,search_clicks_P_alphas = c(0.5, 3)
,search_clicks_P_gammas = c(0.3, 1)
,search_clicks_P_thetas = c(0, 0.3)
#,search_clicks_P_shapes = c(0.0001, 2)
#,search_clicks_P_scales = c(0, 0.1)
,ooh_S_alphas = c(0.5, 3)
,ooh_S_gammas = c(0.3, 1)
,ooh_S_thetas = c(0.1, 0.4)
#,ooh_S_shapes = c(0.0001, 2)
#,ooh_S_scales = c(0, 0.1)
,newsletter_alphas = c(0.5, 3)
,newsletter_gammas = c(0.3, 1)
,newsletter_thetas = c(0.1, 0.4)
#,newsletter_shapes = c(0.0001, 2)
#,newsletter_scales = c(0, 0.1)
)
InputCollect <- robyn_inputs(InputCollect = InputCollect, hyperparameters = hyperparameters)
OutputCollect <- robyn_run(
InputCollect = InputCollect # feed in all model specification
, plot_folder = robyn_object # plots will be saved in the same folder as robyn_object.
# Other paths are also possible
, pareto_fronts = 1 # How many Pareto fronts to be exported as model output. The higher,
# the more model results/ one-pagers provided
, plot_pareto = TRUE # Disabling plat_pareto speeds up processing time in the end,
# but plots won't be saved
)
library(Robyn)
library(Robyn)
